
####################################
#Machine Learining algorithm

#1. The series are split into a series of training sets and test sets, with each
#training set comprising the first p < n observations (for p = q, q + 1, . . . , n − 1)
#and the corresponding test set comprising only the observations at time p + 1.
set.seed(5)
inds <- partition(AT$AAA, p = c(train = 0.75, test = 0.25), type = "blocked")
train <- AT[inds$train, 3:78]
test <- AT[inds$test, 3:78]

#2. A forecasting model is fitted to each series in each training set and one-step-
#ahead forecasts are produced for each test set.
fit <- lapply(1:76, function(i) auto.arima(train[,i]))
forecasts <- lapply(1:76, function(i) forecast(test[,i], h = 1, model = fit[[i]]))

#3. A separate ML model (either a RF or XGB) is built for predicting each of the
#mk bottom series of the hierarchy. The training set of each model consists
#of n − p observations and m + 1 variables. The first m variables (used as
#predictors or inputs) are the one-step ahead forecasts produced during the
#rolling origin process for the m series of the hierarchy, and the last variable
#(the response or target) is the actual value of the bottom-level series at the
#corresponding times. The loss function of the models is the sum of squared
#errors, and the hyper-parameters of the ML models are determined either
#arbitrarily by the user or through an optimization procedure.
RF <- randomForest(AT, ntree = 100, mtry = 4, nodesize = 35)
predict.randomForest(RF, forecasts[c(1:76)])
